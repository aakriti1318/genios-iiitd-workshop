{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c76983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc62656a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcef131b",
   "metadata": {},
   "source": [
    "## 1. Pull an article from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30051cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    paragraphs = [p.get_text() for p in soup.find_all(\"p\")]\n",
    "    return \"\\n\".join(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e48720",
   "metadata": {},
   "source": [
    "#### Example article on \"Research with RAG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc6f98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_url = \"https://aws.amazon.com/what-is/retrieval-augmented-generation/\"  # Replace with real URL\n",
    "article_text = get_article_text(article_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aeb714",
   "metadata": {},
   "source": [
    "## 2. Chunk the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1644be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "chunks = text_splitter.split_text(article_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfbaccc",
   "metadata": {},
   "source": [
    "## 3. Store in vector DB (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c46832d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "vectorstore = FAISS.from_texts(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b06190",
   "metadata": {},
   "source": [
    "## 4. Query and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43a5b11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vq/krc_rfjj2md033j93jx4c3240000gn/T/ipykernel_97397/98592025.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0, model_name=\"gpt-4.1-mini\")\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd2ab1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441e47fa",
   "metadata": {},
   "source": [
    "#### Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ea98199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: RAG (Retrieval-Augmented Generation) improves research efficiency by enabling large language models (LLMs) to retrieve and incorporate relevant, authoritative information from pre-determined knowledge sources during the generation process. This approach offers several benefits that enhance research workflows:\n",
      "\n",
      "1. **Accurate and Relevant Information:** RAG directs the LLM to access up-to-date and specific data from trusted sources, ensuring that the generated content is accurate and pertinent to the research query.\n",
      "\n",
      "2. **Source Attribution:** The output can include citations or references, allowing researchers to verify information easily and consult original documents for deeper understanding, which saves time in cross-checking facts.\n",
      "\n",
      "3. **Access to Latest Data:** By connecting LLMs to live or frequently updated sources such as news sites or social media feeds, RAG ensures that researchers receive the most current information without manually searching multiple platforms.\n",
      "\n",
      "4. **Controlled Knowledge Base:** Organizations can curate and maintain the knowledge sources used by RAG, ensuring that the research is based on relevant and high-quality materials tailored to specific domains.\n",
      "\n",
      "Overall, RAG streamlines the research process by combining the generative capabilities of LLMs with precise retrieval of authoritative information, reducing the time and effort needed to gather, verify, and synthesize data.\n",
      "\n",
      "Sources:\n",
      "- RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control  ...\n",
      "- RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they requir ...\n",
      "- Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to ...\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain how RAG improves research efficiency.\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.page_content[:200], \"...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
